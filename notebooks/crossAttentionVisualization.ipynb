{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb9e474b-b35d-4d0a-a9aa-9a73e2744e25",
   "metadata": {},
   "source": [
    "# Encoder-Decoder Attention Weights Visualisation for Audio Captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6da9b16f-c51e-4d2d-bb9c-17639d50f40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_DATA_DIR = \"/home/akhil/models/DCASE24/dcase2024-task6-baseline/data/CLOTHO_v2.1/clotho_audio_files/evaluation\"\n",
    "EVAL_RESULTS_META = \"/home/akhil/models/DCASE24/dcase2024-task6-baseline/logs/test-2024.04.23-20.20.00/test_clotho_eval_outputs.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a87d6999-2346-4c18-9c54-ff1ed174521a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns available :  Index(['test/loss', 'predictions', 'log_probs', 'beam_predictions',\n",
      "       'beam_log_probs', 'candidates', 'beam_candidates', 'subset',\n",
      "       'mult_captions', 'mult_references', 'fname', 'dataset',\n",
      "       'dataloader_idx', 'batch_idx', 'stage', 'bleu_1', 'bleu_2', 'bleu_3',\n",
      "       'bleu_4', 'meteor', 'rouge_l', 'sbert_sim', 'fer', 'fense', 'cider_d',\n",
      "       'spice', 'spider', 'fer.add_tail_prob', 'fer.repeat_event_prob',\n",
      "       'fer.repeat_adv_prob', 'fer.remove_conj_prob', 'fer.remove_verb_prob',\n",
      "       'fer.error_prob', 'spider_fl', 'bert_score.precision',\n",
      "       'bert_score.recall', 'bert_score.f1'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(EVAL_RESULTS_META)\n",
    "print(\"Columns available : \", df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dd5fb0-eaaf-4603-9751-234563176ed4",
   "metadata": {},
   "source": [
    "## Preparing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a25b2c0-c429-4520-9ba9-f99c918e3e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dcase24t6.nn.hub import baseline_pipeline\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch import nn\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual,widgets\n",
    "import IPython\n",
    "import matplotlib._color_data as mcd\n",
    "import librosa\n",
    "import os\n",
    "\n",
    "font = {'weight' : 'bold', 'size' : 18}\n",
    "matplotlib.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "293629ac-8dec-4d33-8923-cb1af3349b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    fig = plt.figure(figsize=(20, 6), dpi=80)\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.cpu().numpy(), cmap='gray_r')\n",
    "    \n",
    "    # Set up axes\n",
    "    ax.set_xticks(range(attentions.shape[-1]))\n",
    "    ax.set_yticks(range(attentions.shape[-2]))\n",
    "    ax.set_xticklabels([item*0.3125 for item in range(1, attentions.shape[-1]+1)], rotation=90)\n",
    "    ax.set_yticklabels(output_words+[\"<EOS>\"])\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def plotAttention(attention, output_words, ax=None):\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(1, 1)\n",
    "\n",
    "    y_ticks = output_words.split(\" \")+[\"<EOS>\"]\n",
    "    attention = attention[:len(y_ticks), :]\n",
    "    \n",
    "    cax = ax.matshow(attention, cmap='gray_r')\n",
    "\n",
    "    ax.set_xticks(range(attention.shape[-1]))\n",
    "    ax.set_yticks(range(attention.shape[-2]))\n",
    "    ax.set_xticklabels([item*0.3125 for item in range(1, attention.shape[-1]+1)], rotation=90)\n",
    "    ax.set_yticklabels(output_words.split(\" \")+[\"<EOS>\"])\n",
    "\n",
    "# Getting attention maps\n",
    "def patch_attention(m):\n",
    "    forward_orig = m.forward\n",
    "\n",
    "    def wrap(*args, **kwargs):\n",
    "        kwargs[\"need_weights\"] = True\n",
    "        kwargs[\"average_attn_weights\"] = False\n",
    "\n",
    "        return forward_orig(*args, **kwargs)\n",
    "\n",
    "    m.forward = wrap\n",
    "    \n",
    "class SaveOutput:\n",
    "    def __init__(self):\n",
    "        self.outputs = []\n",
    "\n",
    "    def __call__(self, module, module_in, module_out):\n",
    "        self.outputs.append(module_out[1])\n",
    "\n",
    "    def clear(self):\n",
    "        self.outputs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e282f5b6-adc0-4492-a0cb-f7c57b7ac978",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/akhil/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading similarity matrix:\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for TransDecoderModel:\n\tMissing key(s) in state_dict: \"keyword_enc.encoder.layers.0.self_attn.in_proj_weight\", \"keyword_enc.encoder.layers.0.self_attn.in_proj_bias\", \"keyword_enc.encoder.layers.0.self_attn.out_proj.weight\", \"keyword_enc.encoder.layers.0.self_attn.out_proj.bias\", \"keyword_enc.encoder.layers.0.linear1.weight\", \"keyword_enc.encoder.layers.0.linear1.bias\", \"keyword_enc.encoder.layers.0.linear2.weight\", \"keyword_enc.encoder.layers.0.linear2.bias\", \"keyword_enc.encoder.layers.0.norm1.weight\", \"keyword_enc.encoder.layers.0.norm1.bias\", \"keyword_enc.encoder.layers.0.norm2.weight\", \"keyword_enc.encoder.layers.0.norm2.bias\", \"keyword_enc.encoder.layers.1.self_attn.in_proj_weight\", \"keyword_enc.encoder.layers.1.self_attn.in_proj_bias\", \"keyword_enc.encoder.layers.1.self_attn.out_proj.weight\", \"keyword_enc.encoder.layers.1.self_attn.out_proj.bias\", \"keyword_enc.encoder.layers.1.linear1.weight\", \"keyword_enc.encoder.layers.1.linear1.bias\", \"keyword_enc.encoder.layers.1.linear2.weight\", \"keyword_enc.encoder.layers.1.linear2.bias\", \"keyword_enc.encoder.layers.1.norm1.weight\", \"keyword_enc.encoder.layers.1.norm1.bias\", \"keyword_enc.encoder.layers.1.norm2.weight\", \"keyword_enc.encoder.layers.1.norm2.bias\", \"keyword_enc.encoder.layers.2.self_attn.in_proj_weight\", \"keyword_enc.encoder.layers.2.self_attn.in_proj_bias\", \"keyword_enc.encoder.layers.2.self_attn.out_proj.weight\", \"keyword_enc.encoder.layers.2.self_attn.out_proj.bias\", \"keyword_enc.encoder.layers.2.linear1.weight\", \"keyword_enc.encoder.layers.2.linear1.bias\", \"keyword_enc.encoder.layers.2.linear2.weight\", \"keyword_enc.encoder.layers.2.linear2.bias\", \"keyword_enc.encoder.layers.2.norm1.weight\", \"keyword_enc.encoder.layers.2.norm1.bias\", \"keyword_enc.encoder.layers.2.norm2.weight\", \"keyword_enc.encoder.layers.2.norm2.bias\", \"keyword_enc.encoder.layers.3.self_attn.in_proj_weight\", \"keyword_enc.encoder.layers.3.self_attn.in_proj_bias\", \"keyword_enc.encoder.layers.3.self_attn.out_proj.weight\", \"keyword_enc.encoder.layers.3.self_attn.out_proj.bias\", \"keyword_enc.encoder.layers.3.linear1.weight\", \"keyword_enc.encoder.layers.3.linear1.bias\", \"keyword_enc.encoder.layers.3.linear2.weight\", \"keyword_enc.encoder.layers.3.linear2.bias\", \"keyword_enc.encoder.layers.3.norm1.weight\", \"keyword_enc.encoder.layers.3.norm1.bias\", \"keyword_enc.encoder.layers.3.norm2.weight\", \"keyword_enc.encoder.layers.3.norm2.bias\", \"keyword_enc.embedding_layer.weight\", \"keyword_enc.classifier.weight\", \"keyword_enc.classifier.bias\", \"keyword_enc.projection.weight\", \"keyword_enc.projection.bias\", \"decoder.classifier.projection_layers.0.weight\", \"decoder.classifier.projection_layers.0.bias\", \"decoder.classifier.projection_layers.1.weight\", \"decoder.classifier.projection_layers.1.bias\", \"decoder.classifier.projection_layers.2.weight\", \"decoder.classifier.projection_layers.2.bias\", \"decoder.classifier.projection_layers.3.weight\", \"decoder.classifier.projection_layers.3.bias\", \"decoder.classifier.router.weight\", \"decoder.classifier.router.bias\". \n\tUnexpected key(s) in state_dict: \"decoder.classifier.weight\", \"decoder.classifier.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mbaseline_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/models/DCASE24/dcase2024-task6-baseline/src/dcase24t6/nn/hub.py:56\u001b[0m, in \u001b[0;36mbaseline_pipeline\u001b[0;34m(model_name_or_path, tokenizer_name_or_path, pre_process_name_or_path, offline, device, verbose)\u001b[0m\n\u001b[1;32m     53\u001b[0m         BASELINE_REGISTRY\u001b[38;5;241m.\u001b[39mdownload_file(tokenizer_name, verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[1;32m     55\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AACTokenizer\u001b[38;5;241m.\u001b[39mfrom_file(tokenizer_path)\n\u001b[0;32m---> 56\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mTransDecoderModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(pre_process, model)\n\u001b[1;32m     58\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[0;32m~/models/DCASE24/dcase2024-task6-baseline/aac_venv/lib/python3.11/site-packages/lightning/pytorch/utilities/model_helpers.py:125\u001b[0m, in \u001b[0;36m_restricted_classmethod_impl.__get__.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m instance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scripting:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe classmethod `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` cannot be called on an instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please call it on the class type and make sure the return value is used.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m     )\n\u001b[0;32m--> 125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/models/DCASE24/dcase2024-task6-baseline/aac_venv/lib/python3.11/site-packages/lightning/pytorch/core/module.py:1581\u001b[0m, in \u001b[0;36mLightningModule.load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m   1492\u001b[0m \u001b[38;5;129m@_restricted_classmethod\u001b[39m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_from_checkpoint\u001b[39m(\n\u001b[1;32m   1494\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1499\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1500\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;124;03m    passed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1579\u001b[0m \n\u001b[1;32m   1580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1581\u001b[0m     loaded \u001b[38;5;241m=\u001b[39m \u001b[43m_load_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1582\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhparams_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1587\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1588\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1589\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Self, loaded)\n",
      "File \u001b[0;32m~/models/DCASE24/dcase2024-task6-baseline/aac_venv/lib/python3.11/site-packages/lightning/pytorch/core/saving.py:91\u001b[0m, in \u001b[0;36m_load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _load_state(\u001b[38;5;28mcls\u001b[39m, checkpoint, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, pl\u001b[38;5;241m.\u001b[39mLightningModule):\n\u001b[0;32m---> 91\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m state_dict:\n",
      "File \u001b[0;32m~/models/DCASE24/dcase2024-task6-baseline/aac_venv/lib/python3.11/site-packages/lightning/pytorch/core/saving.py:180\u001b[0m, in \u001b[0;36m_load_state\u001b[0;34m(cls, checkpoint, strict, **cls_kwargs_new)\u001b[0m\n\u001b[1;32m    177\u001b[0m     obj\u001b[38;5;241m.\u001b[39mon_load_checkpoint(checkpoint)\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m# load the state_dict on the model automatically\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m keys \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstate_dict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m strict:\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m keys\u001b[38;5;241m.\u001b[39mmissing_keys:\n",
      "File \u001b[0;32m~/models/DCASE24/dcase2024-task6-baseline/aac_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for TransDecoderModel:\n\tMissing key(s) in state_dict: \"keyword_enc.encoder.layers.0.self_attn.in_proj_weight\", \"keyword_enc.encoder.layers.0.self_attn.in_proj_bias\", \"keyword_enc.encoder.layers.0.self_attn.out_proj.weight\", \"keyword_enc.encoder.layers.0.self_attn.out_proj.bias\", \"keyword_enc.encoder.layers.0.linear1.weight\", \"keyword_enc.encoder.layers.0.linear1.bias\", \"keyword_enc.encoder.layers.0.linear2.weight\", \"keyword_enc.encoder.layers.0.linear2.bias\", \"keyword_enc.encoder.layers.0.norm1.weight\", \"keyword_enc.encoder.layers.0.norm1.bias\", \"keyword_enc.encoder.layers.0.norm2.weight\", \"keyword_enc.encoder.layers.0.norm2.bias\", \"keyword_enc.encoder.layers.1.self_attn.in_proj_weight\", \"keyword_enc.encoder.layers.1.self_attn.in_proj_bias\", \"keyword_enc.encoder.layers.1.self_attn.out_proj.weight\", \"keyword_enc.encoder.layers.1.self_attn.out_proj.bias\", \"keyword_enc.encoder.layers.1.linear1.weight\", \"keyword_enc.encoder.layers.1.linear1.bias\", \"keyword_enc.encoder.layers.1.linear2.weight\", \"keyword_enc.encoder.layers.1.linear2.bias\", \"keyword_enc.encoder.layers.1.norm1.weight\", \"keyword_enc.encoder.layers.1.norm1.bias\", \"keyword_enc.encoder.layers.1.norm2.weight\", \"keyword_enc.encoder.layers.1.norm2.bias\", \"keyword_enc.encoder.layers.2.self_attn.in_proj_weight\", \"keyword_enc.encoder.layers.2.self_attn.in_proj_bias\", \"keyword_enc.encoder.layers.2.self_attn.out_proj.weight\", \"keyword_enc.encoder.layers.2.self_attn.out_proj.bias\", \"keyword_enc.encoder.layers.2.linear1.weight\", \"keyword_enc.encoder.layers.2.linear1.bias\", \"keyword_enc.encoder.layers.2.linear2.weight\", \"keyword_enc.encoder.layers.2.linear2.bias\", \"keyword_enc.encoder.layers.2.norm1.weight\", \"keyword_enc.encoder.layers.2.norm1.bias\", \"keyword_enc.encoder.layers.2.norm2.weight\", \"keyword_enc.encoder.layers.2.norm2.bias\", \"keyword_enc.encoder.layers.3.self_attn.in_proj_weight\", \"keyword_enc.encoder.layers.3.self_attn.in_proj_bias\", \"keyword_enc.encoder.layers.3.self_attn.out_proj.weight\", \"keyword_enc.encoder.layers.3.self_attn.out_proj.bias\", \"keyword_enc.encoder.layers.3.linear1.weight\", \"keyword_enc.encoder.layers.3.linear1.bias\", \"keyword_enc.encoder.layers.3.linear2.weight\", \"keyword_enc.encoder.layers.3.linear2.bias\", \"keyword_enc.encoder.layers.3.norm1.weight\", \"keyword_enc.encoder.layers.3.norm1.bias\", \"keyword_enc.encoder.layers.3.norm2.weight\", \"keyword_enc.encoder.layers.3.norm2.bias\", \"keyword_enc.embedding_layer.weight\", \"keyword_enc.classifier.weight\", \"keyword_enc.classifier.bias\", \"keyword_enc.projection.weight\", \"keyword_enc.projection.bias\", \"decoder.classifier.projection_layers.0.weight\", \"decoder.classifier.projection_layers.0.bias\", \"decoder.classifier.projection_layers.1.weight\", \"decoder.classifier.projection_layers.1.bias\", \"decoder.classifier.projection_layers.2.weight\", \"decoder.classifier.projection_layers.2.bias\", \"decoder.classifier.projection_layers.3.weight\", \"decoder.classifier.projection_layers.3.bias\", \"decoder.classifier.router.weight\", \"decoder.classifier.router.bias\". \n\tUnexpected key(s) in state_dict: \"decoder.classifier.weight\", \"decoder.classifier.bias\". "
     ]
    }
   ],
   "source": [
    "model = baseline_pipeline()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ac6b8f8-e06e-4400-9253-658aac52dfc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9af6980902304abea97b8aa2cab9ac57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='filename', options=('Santa Motor.wav', 'Radio Garble.wav', 'Radio …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SAMPLE_RATE = 32000\n",
    "\n",
    "@interact\n",
    "def ref_sound_selector(filename = df['fname'], decoder_layer = [-1, 0, 1, 2, 3, 4, 5]):\n",
    "    filepath = os.path.join(EVAL_DATA_DIR, filename)\n",
    "\n",
    "    input_signal, sr = librosa.core.load(filepath, sr = SAMPLE_RATE, mono=True)\n",
    "    \n",
    "    print(\"\\nInput Signal | fs :\", sr)\n",
    "    IPython.display.display(IPython.display.Audio(input_signal, rate=sr))\n",
    "\n",
    "    instances_df = df[df['fname'] == filename]\n",
    "    print(\"\\nReference Captions : \", instances_df[\"mult_references\"].item())\n",
    "\n",
    "    # Get attention weights\n",
    "    audio = torch.tensor(input_signal)[None, :]\n",
    "    item = {\"audio\": audio, \"sr\": sr}\n",
    "\n",
    "    save_output = SaveOutput()\n",
    "    patch_attention(model[1].decoder.layers[decoder_layer].multihead_attn)\n",
    "    hook_handle = model[1].decoder.layers[decoder_layer].multihead_attn.register_forward_hook(save_output)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(item)\n",
    "\n",
    "    print(\"\\nPredicted Caption : \", outputs[\"candidates\"][0])\n",
    "    print(\"\\nFENSE Score : \", instances_df[\"fense\"].item())\n",
    "\n",
    "    attn_weights = save_output.outputs[-1].cpu().numpy()[0]\n",
    "\n",
    "    fig, axs = plt.subplots(4, 2, figsize=(40, 30), dpi=80)\n",
    "    axs = np.array(axs)\n",
    "\n",
    "    for index, ax in enumerate(axs.reshape(-1)):\n",
    "        ax.set_title(\"Head \" + str(index))\n",
    "        plotAttention(attn_weights[index], outputs[\"candidates\"][0], ax)\n",
    "    \n",
    "    # fig.tight_layout()\n",
    "    # fig.subplots_adjust(hspace=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d78542e-07e0-43f9-af08-9be234732c95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
