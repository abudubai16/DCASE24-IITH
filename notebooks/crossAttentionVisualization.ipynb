{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb9e474b-b35d-4d0a-a9aa-9a73e2744e25",
   "metadata": {},
   "source": [
    "# Encoder-Decoder Attention Weights Visualisation for Audio Captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6da9b16f-c51e-4d2d-bb9c-17639d50f40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_DATA_DIR = \"/home/akhil/models/DCASE24/dcase2024-task6-baseline/data/CLOTHO_v2.1/clotho_audio_files/evaluation\"\n",
    "EVAL_RESULTS_META = \"/home/akhil/models/DCASE24/dcase2024-task6-baseline/logs/test-2024.04.23-20.20.00/test_clotho_eval_outputs.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a87d6999-2346-4c18-9c54-ff1ed174521a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns available :  Index(['test/loss', 'predictions', 'log_probs', 'beam_predictions',\n",
      "       'beam_log_probs', 'candidates', 'beam_candidates', 'subset',\n",
      "       'mult_captions', 'mult_references', 'fname', 'dataset',\n",
      "       'dataloader_idx', 'batch_idx', 'stage', 'bleu_1', 'bleu_2', 'bleu_3',\n",
      "       'bleu_4', 'meteor', 'rouge_l', 'sbert_sim', 'fer', 'fense', 'cider_d',\n",
      "       'spice', 'spider', 'fer.add_tail_prob', 'fer.repeat_event_prob',\n",
      "       'fer.repeat_adv_prob', 'fer.remove_conj_prob', 'fer.remove_verb_prob',\n",
      "       'fer.error_prob', 'spider_fl', 'bert_score.precision',\n",
      "       'bert_score.recall', 'bert_score.f1'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(EVAL_RESULTS_META)\n",
    "print(\"Columns available : \", df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dd5fb0-eaaf-4603-9751-234563176ed4",
   "metadata": {},
   "source": [
    "## Preparing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a25b2c0-c429-4520-9ba9-f99c918e3e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dcase24t6.nn.hub import baseline_pipeline\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch import nn\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual,widgets\n",
    "import IPython\n",
    "import matplotlib._color_data as mcd\n",
    "import librosa\n",
    "import os\n",
    "\n",
    "font = {'weight' : 'bold', 'size' : 18}\n",
    "matplotlib.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "293629ac-8dec-4d33-8923-cb1af3349b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    fig = plt.figure(figsize=(20, 6), dpi=80)\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.cpu().numpy(), cmap='gray_r')\n",
    "    \n",
    "    # Set up axes\n",
    "    ax.set_xticks(range(attentions.shape[-1]))\n",
    "    ax.set_yticks(range(attentions.shape[-2]))\n",
    "    ax.set_xticklabels([item*0.3125 for item in range(1, attentions.shape[-1]+1)], rotation=90)\n",
    "    ax.set_yticklabels(output_words+[\"<EOS>\"])\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def plotAttention(attention, output_words, ax=None):\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(1, 1)\n",
    "\n",
    "    y_ticks = output_words.split(\" \")+[\"<EOS>\"]\n",
    "    attention = attention[:len(y_ticks), :]\n",
    "    \n",
    "    cax = ax.matshow(attention, cmap='gray_r')\n",
    "\n",
    "    ax.set_xticks(range(attention.shape[-1]))\n",
    "    ax.set_yticks(range(attention.shape[-2]))\n",
    "    ax.set_xticklabels([item*0.3125 for item in range(1, attention.shape[-1]+1)], rotation=90)\n",
    "    ax.set_yticklabels(output_words.split(\" \")+[\"<EOS>\"])\n",
    "\n",
    "# Getting attention maps\n",
    "def patch_attention(m):\n",
    "    forward_orig = m.forward\n",
    "\n",
    "    def wrap(*args, **kwargs):\n",
    "        kwargs[\"need_weights\"] = True\n",
    "        kwargs[\"average_attn_weights\"] = False\n",
    "\n",
    "        return forward_orig(*args, **kwargs)\n",
    "\n",
    "    m.forward = wrap\n",
    "    \n",
    "class SaveOutput:\n",
    "    def __init__(self):\n",
    "        self.outputs = []\n",
    "\n",
    "    def __call__(self, module, module_in, module_out):\n",
    "        self.outputs.append(module_out[1])\n",
    "\n",
    "    def clear(self):\n",
    "        self.outputs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e282f5b6-adc0-4492-a0cb-f7c57b7ac978",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/akhil/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): ResampleMeanCNext(\n",
       "    (convnext): ConvNeXt(\n",
       "      (spectrogram_extractor): Spectrogram(\n",
       "        (stft): STFT(\n",
       "          (conv_real): Conv1d(1, 513, kernel_size=(1024,), stride=(320,), bias=False)\n",
       "          (conv_imag): Conv1d(1, 513, kernel_size=(1024,), stride=(320,), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (logmel_extractor): LogmelFilterBank()\n",
       "      (spec_augmenter): SpecAugmentation(\n",
       "        (time_dropper): DropStripes()\n",
       "        (freq_dropper): DropStripes()\n",
       "      )\n",
       "      (speed_perturb): Identity()\n",
       "      (bn0): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample_layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): Conv2d(1, 96, kernel_size=(4, 4), stride=(4, 4), padding=(4, 0))\n",
       "          (1): CustomLayerNorm()\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): CustomLayerNorm()\n",
       "          (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): CustomLayerNorm()\n",
       "          (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n",
       "        )\n",
       "        (3): Sequential(\n",
       "          (0): CustomLayerNorm()\n",
       "          (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n",
       "        )\n",
       "      )\n",
       "      (stages): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): CNextBlock(\n",
       "            (dwconv): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "            (norm): CustomLayerNorm()\n",
       "            (pwconv1): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (pwconv2): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (1): CNextBlock(\n",
       "            (dwconv): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "            (norm): CustomLayerNorm()\n",
       "            (pwconv1): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (pwconv2): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (2): CNextBlock(\n",
       "            (dwconv): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "            (norm): CustomLayerNorm()\n",
       "            (pwconv1): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (pwconv2): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): CNextBlock(\n",
       "            (dwconv): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "            (norm): CustomLayerNorm()\n",
       "            (pwconv1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (pwconv2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (1): CNextBlock(\n",
       "            (dwconv): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "            (norm): CustomLayerNorm()\n",
       "            (pwconv1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (pwconv2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (2): CNextBlock(\n",
       "            (dwconv): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "            (norm): CustomLayerNorm()\n",
       "            (pwconv1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (pwconv2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): CNextBlock(\n",
       "            (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (norm): CustomLayerNorm()\n",
       "            (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (1): CNextBlock(\n",
       "            (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (norm): CustomLayerNorm()\n",
       "            (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (2): CNextBlock(\n",
       "            (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (norm): CustomLayerNorm()\n",
       "            (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (3): CNextBlock(\n",
       "            (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (norm): CustomLayerNorm()\n",
       "            (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (4): CNextBlock(\n",
       "            (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (norm): CustomLayerNorm()\n",
       "            (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (5): CNextBlock(\n",
       "            (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (norm): CustomLayerNorm()\n",
       "            (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (6): CNextBlock(\n",
       "            (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (norm): CustomLayerNorm()\n",
       "            (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (7): CNextBlock(\n",
       "            (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (norm): CustomLayerNorm()\n",
       "            (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (8): CNextBlock(\n",
       "            (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (norm): CustomLayerNorm()\n",
       "            (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "        )\n",
       "        (3): Sequential(\n",
       "          (0): CNextBlock(\n",
       "            (dwconv): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "            (norm): CustomLayerNorm()\n",
       "            (pwconv1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (pwconv2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (1): CNextBlock(\n",
       "            (dwconv): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "            (norm): CustomLayerNorm()\n",
       "            (pwconv1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (pwconv2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (2): CNextBlock(\n",
       "            (dwconv): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "            (norm): CustomLayerNorm()\n",
       "            (pwconv1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (pwconv2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (head_audioset): Linear(in_features=768, out_features=527, bias=True)\n",
       "    )\n",
       "    (resample): Resample()\n",
       "  )\n",
       "  (1): TransDecoderModel(\n",
       "    (projection): Sequential(\n",
       "      (0): Dropout(p=0.5, inplace=False)\n",
       "      (1): Transpose(1, 2)\n",
       "      (2): Linear(in_features=768, out_features=256, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): Transpose(1, 2)\n",
       "      (5): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (decoder): AACTransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.2, inplace=False)\n",
       "          (dropout2): Dropout(p=0.2, inplace=False)\n",
       "          (dropout3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (emb_layer): Embedding(4371, 256, padding_idx=0)\n",
       "      (pos_encoding): PositionalEncoding(\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (classifier): Linear(in_features=256, out_features=4371, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = baseline_pipeline()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ac6b8f8-e06e-4400-9253-658aac52dfc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9af6980902304abea97b8aa2cab9ac57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='filename', options=('Santa Motor.wav', 'Radio Garble.wav', 'Radio â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SAMPLE_RATE = 32000\n",
    "\n",
    "@interact\n",
    "def ref_sound_selector(filename = df['fname'], decoder_layer = [-1, 0, 1, 2, 3, 4, 5]):\n",
    "    filepath = os.path.join(EVAL_DATA_DIR, filename)\n",
    "\n",
    "    input_signal, sr = librosa.core.load(filepath, sr = SAMPLE_RATE, mono=True)\n",
    "    \n",
    "    print(\"\\nInput Signal | fs :\", sr)\n",
    "    IPython.display.display(IPython.display.Audio(input_signal, rate=sr))\n",
    "\n",
    "    instances_df = df[df['fname'] == filename]\n",
    "    print(\"\\nReference Captions : \", instances_df[\"mult_references\"].item())\n",
    "\n",
    "    # Get attention weights\n",
    "    audio = torch.tensor(input_signal)[None, :]\n",
    "    item = {\"audio\": audio, \"sr\": sr}\n",
    "\n",
    "    save_output = SaveOutput()\n",
    "    patch_attention(model[1].decoder.layers[decoder_layer].multihead_attn)\n",
    "    hook_handle = model[1].decoder.layers[decoder_layer].multihead_attn.register_forward_hook(save_output)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(item)\n",
    "\n",
    "    print(\"\\nPredicted Caption : \", outputs[\"candidates\"][0])\n",
    "    print(\"\\nFENSE Score : \", instances_df[\"fense\"].item())\n",
    "\n",
    "    attn_weights = save_output.outputs[-1].cpu().numpy()[0]\n",
    "\n",
    "    fig, axs = plt.subplots(4, 2, figsize=(40, 30), dpi=80)\n",
    "    axs = np.array(axs)\n",
    "\n",
    "    for index, ax in enumerate(axs.reshape(-1)):\n",
    "        ax.set_title(\"Head \" + str(index))\n",
    "        plotAttention(attn_weights[index], outputs[\"candidates\"][0], ax)\n",
    "    \n",
    "    # fig.tight_layout()\n",
    "    # fig.subplots_adjust(hspace=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d78542e-07e0-43f9-af08-9be234732c95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
